\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr} % Required for custom headers
%\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{ulem} % underline emph
\usepackage{amsmath} % for \text in mathmode
\usepackage[hypcap]{caption}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0.5in
\textwidth=5.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.3} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{} % Top left header
\chead{\hmwkClass: \hmwkTitle} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{\thepage} % Bottom center footer
%\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Matlab} % Load C syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=C, % Use python in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % C functions bold and blue
        keywordstyle=[2]\color{Purple}, % C function arguments purple
        keywordstyle=[3]\color{Blue}, % Custom functions \underbar underlined and blue
        identifierstyle=, % Nothing special about identifiers
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Python functions not included in the default language here
        morekeywords={rand},
        %
        % Put Python function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{glutCreateWindow,p},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=none, % can use none % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=1 % Line numbers go in steps of 5
}
% \usepackage{graphicx}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\code}[1]{
\begin{itemize}
\item[]\lstinputlisting[label=#1]{#1.c}
%\item[]\lstinputlisting[caption=#2,label=#1]{#1.c}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

\setcounter{secnumdepth}{0} % Removes default section numbers

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
    \renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
    \section{\homeworkProblemName} % Make a section in the document with the custom problem count
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
    \noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
    \renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
    \subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{\\PI Approximation} % Assignment title
\newcommand{\hmwkDueDate}{\date{March 08, 2017}} % Due date
\newcommand{\hmwkClass}{Introduction to Supercomputing\\TMA4280} % Course/class
\newcommand{\hmwkAuthorName}{Neshat\ Naderi}  % Your name


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\normalsize{\hmwkDueDate}
\vspace{0.1in}\large{\text{Compiler Construction}}
\vspace{3in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------
\begin{document}
\maketitle

% \setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

% \newpage
% \tableofcontents
% \newpage

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem
\clearpage

\section{}
\begin{homeworkProblem}{Problem Solving Approach}

\subsection{Single-process program}
The vector $v$ is generated by first declaring a pointer $*v$ and allocating memory with \texttt{malloc}. The size of vector is given as a user input to the program. The allocated memory was initialized before assigning the vector elements to avoid garbage data.\\
The summing function \texttt{compute\_sum} computed $S_n$ by iterating over vector elements using a for-loop. The error of the sum result computed by subtracting $S_n$ from $S$. The absolute value of the subtraction was returned by the \texttt{math} function \texttt{fabs()}.

\subsection{Parallel program using OpenMP}
The first step to change the single-process program to a parallelized program in OpenMP was including \texttt{omp.h}.
The loop statement in \texttt{compute\_sum} was defined as a parallel region as below.\\
\begin{lstlisting}
    #pragma omp parallel for reduction(+:s)
    for (int i = 0; i < n; i++) {
        s += v[i];
    }
\end{lstlisting}

The compiler directive \texttt{omp} \texttt{parallel} \texttt{for} \texttt{reduction} partitions the loop evenly. When all the threads have found their partial sum, the sum can be reduced to the shared variable \texttt{sum}.

\subsection{Parallel program using MPI}
The library header file \texttt{mpi.h} was included to use MPI definitions and MPI data types. \texttt{MPI\_Init} was called to initialize the execution environment. The number of processes, \texttt{nprocs}, was set as a compiler flag. The size of the problem, \texttt{n}, was given as a command line argument, and was only read by rank 0.
After initializing, \texttt{n} was sent to all ranks using \texttt{MPI\_Broadcast}. The entire vector $v$ is generated by rank 0 alone. Afterwards, rank 0 partitions the vector into chunks with size $\frac{n}{nprocs}$, and distributes them evenly across all ranks. This was done with \texttt{MPI\_Scatter}. After the partial sum was computed by each process, the final sum was computed by collecting and summing using \texttt{MPI\_Reduce} and \texttt{MPI\_SUM} as an argument.
\texttt{MPI\_Finalize()} was called to terminate the MPI execution environment.
\clearpage
\begin{lstlisting}
    // initialize MPI environment
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);

    // Message passing call
    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Scatter(v, chunk_size, MPI_DOUBLE,
        scattered_v, chunk_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);

    MPI_Reduce(&partial_sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM,
        0, MPI_COMM_WORLD);

    // Terminate MPI environment
    MPI_Finalize();
\end{lstlisting}

% \subsection{Parallelization using both OpenMP and MPI}
% Combining the two was simple and required little extra code. The only thing new in this part was that MPI\_Init was replaced by MPI\_Init\_thread. This initializes the thread environment, which allows the compiler to make some optimizations.
\end{homeworkProblem}

\begin{homeworkProblem}{}
  \begin{homeworkSection}{Unit test}
    Unit tests could be useful for verifying if the results expected from a program is returned correctly.
    So in order to check the output result from a parallel program we could look into the errors of calculation within a
    parallel program comparing to a serial one. Because parallel programs could return different values of floating points operation
    because of the rounding in different stages like partial computation by processes. While in a serial program it is done at the end of the computation.
  \end{homeworkSection}

  \begin{homeworkSection}{Verification test}
  Results for $k = 1,..,24\ $ shows that the error for Reimann Zeta function decreases by increasing the size of $k$.
  The convergence for Machin formula happens very early in computation and it does not change even comparing the summation with $\pi$ value with more than $100$ decimals. So the approximation of Machin formula is more accurate compared to Reimann Zeta.
  The conclusion is that Machin formula needs larger datatypes with more decimals to converge slowly.
  \end{homeworkSection}

  \begin{homeworkSection}{Data distribution}
  Load balancing of the work between processes could be callenging when the computation time could vary for each vector elements.
  In addition if the length of the vector is not dividable by the number of processses the root process could be loaded by more work than others.
  Like in this implementation the root takes care of both data distribution and I\/O in addition to the computational operations. That could prevent the perfect parallel computation performance such that processors wait busy until the root processsors is completed the distribution.
  In this exercise the length is assumed to be power of $2$.
  \end{homeworkSection}

  \begin{homeworkSection}{Analysis}
  ADD PLOTS, ERROR AND TIMING\\~\\
%   Parallel program using MPI is compiled and executed on a WHICH MACHINE with \# number of processors.\\
%   MPI calls were first of all necessary for environment initialisation and termination as listed below:
% \begin{lstlisting}
%     // Initialisation
%     MPI_Init(&argc, &argv);
%     MPI_Comm_rank(MPI_COMM_WORLD, &rank);
%     MPI_Comm_size(MPI_COMM_WORLD, &nproc);
%     .
%     .
%     // Termination
%     MPI_Finalize();
% \end{lstlisting}

% For communication between root process and the the others for distributing the vectors a call to \texttt{MPI\_Scatter} is used. To calculate the final sum from the partial sums calculated by other processes a call to \texttt{MPI\_Reduce} is used. The global sum is the summation of partial sums in the root process. \texttt{MPI\_Bcast} is also called to send the value of $n$ to every process.

\end{homeworkSection}


\begin{homeworkSection}{Discussion}

\subsubsection{Memory requirement}
The primary datatype used in the program is double, which has a size of 64 bits / 8 bytes. For example when $k = 14$, the program allocates memory for $2^{14}=16 384$ doubles. So the total memory used in the single-process program should be approximately $16384\cdot8\text{ bytes} = 131 072\text{ bytes} = 131.1 \text{ kB}$. This is a slight underestimate, as there are other variables in use as well, but the actual memory usage should be in this area. However, the results does not agree.
\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c|c}
         & Single-Process & 8-process 4-threads (Scatter) & 8-process 4-threads (Bcast) \\ \hline
         & 412 kB & 2.2 MB $\cdot$ 8 = 17.6 MB & 2.4 MB $\cdot$ 8 = 19.2 MB
    \end{tabular}
\end{table}

It is expected that the multi-process program should make use of more memory, because it is more complex and includes both MPI and OpenMP code. The memory usage per process was even greater with the naive approach of allocating the entire vector on all processes. As stated previously, the solution distributes the vector evenly using \texttt{MPI\_Scatter}.

\subsubsection{Floating point operations}
Casting an integer to a double is assumed to be a floating point operation. Computing $\frac{1.0}{i^2}$ consists of one division (ignoring type casting). Therefore the \texttt{zeta} program uses $n = 2^k$ floating point operations when generating the vector.

Computing $S_n$ can be separated into two parts, depending on if the program is running on a single process or not. First, there is a loop which iterates over every element in the vector \texttt{v}, and adds the element to the sum. This should amount to a total of \texttt{n+1} floating point operations on the single process program. On the multi-process program, each rank has it's own partial sum, which means that the program at this point has done less summation than the single process program. Assuming that the program has \texttt{P} processes, then the program computing the partial sums will do $n-(P-1)$ floating point operations. The multi-process program still needs to combine the partial sums into a total sum. When using reduction with MPI, one can imagine a tree structure being created where the leaves are partial sums, and the root is the total combined sum. The height of the tree is $\log_2(P)$, and the number of internal nodes is therefore  $\sum_{i=0}^{\log_2(P)-1} 2^i = P-1$. The internal nodes are intermediate sums, and therefore represent floating point operations. Finally, this means that the multi-process program will do $(n-(P-1))+(P-1)$ floating point operations which intuitively enough is just \texttt{n}.
In Machin formula there are $2$ vectors generated. The divivision and power of constant $x$ to $2i-1$ are floating point operations for both vectors. So the number of flops for each vector computation is $2n$. In total it is $4n$ for both single- and multi-process program. \\
To conclude, both the single-process and the multi-process \texttt{zeta} program does $n$ floating point operations and \texttt{mach} program does $4n$ when computing the vectors. And they both do \texttt{n} floating point operations when computing the sum $S_n$.

\subsubsection{Load balancing}
The value of the element $v_i$ gets smaller as \texttt{i} increases. This means that the work required to compute $v_i$ gets larger with \texttt{i}. To counteract this, the program allows OMP to schedule threads in guided mode. In this mode each thread starts out with a large chunk of the loop. As threads finish their work, they are assigned smaller chunks until the loop is finished. This is done to avoid that some threads gets easy chunks and subsequently has to wait for the threads that are dealing with the harder chunks. However, nothing is done to balance the load between processes. The first process gets the easiest chunk, and the last process gets the hardest one. As is the nature of the \texttt{MPI\_Scatter} function. The program is not load balanced.
\end{homeworkSection}

\begin{homeworkSection}{Conclusion}

Parallel programs could make the computation of very large numerical operations easy and possible. If some numerical operations like vector/matrix operations that might take super-polynomial or exponential time, parallelelising would be a solution to run operations simultanously to decrease the time required to finish large computations. In this case both precision and the time is crucial. As we have seen in this project the error differs when executing same program on multi processes compared to single process computation. So if the precision has a major importance, these differences should be considred and analysed for usage in specific projects. \\
In addition to that the number of processors is very important and it should be chosen so that the communicaation overhead does not make the execution slower. By tking these factors into consideration, parallel programming is an interesting and practical method for large and time consuming calculations which I personally find it fascinating.


  \end{homeworkSection}



\end{homeworkProblem}

\end{document}
